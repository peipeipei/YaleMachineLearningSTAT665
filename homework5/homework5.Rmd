---
title: "Homework 5"
author: "Pei Tao"
date: "6/22/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1

```{r}
train <- read.csv("digits_train.csv", as.is = TRUE)
valid <- read.csv("digits_valid.csv", as.is = TRUE)
library(glmnet)
```

1. Apply the lasso penalty to train a multinomial logistic regression classifier with type.multinomial="grouped". Use 10-fold cross-validation on the training set to select your $\lambda$. Show a plot that displays the
misclassification error on the y-axis for values of $\log(\lambda)$ on the x-axis. (Note: The cross-validation
procedure will take some time to run, so get it started, and then go grab a cup of coffee or stretch a bit.)

```{r}
set.seed(665)
g1 <- cv.glmnet(as.matrix(train[,-257]), factor(train[,257]), family="multinomial", alpha=1, type.measure="class", 
                type.multinomial="grouped")
plot(g1)
```

2. Report $\lambda_{min}$ (the $\lambda$ value that achieves the best error/deviance) and $\lambda_{1se}$ (the $\lambda$ value obtained using the one-standard deviation rule). Next, use both $\lambda_{min}$ and $\lambda_{1se}$ to generate predictions on the validation set. Report the resulting misclassification error rates.

```{r}
l1.1se <- g1$lambda.1se
l1.min <- g1$lambda.min
l1.min
l1.1se
```

```{r}
p1.1se <- predict(g1, s=l1.1se, newx = as.matrix(valid[,-257]), type="class")
p1.min <- predict(g1, s=l1.min, newx = as.matrix(valid[,-257]), type="class")

mean(p1.1se != valid[,257])
mean(p1.min != valid[,257])
```

3. For $\lambda = \lambda_{1se}$, how many predictors are selected by glmnet()? How many non-zero coefficients are estimated in total for this value of $\lambda$?

```{r}
l1coeffs <- predict(g1, s=l1.1se, type="coefficients")
#length(l1coeffs)
#sapply(l1coeffs, function(i) sum(i != 0))
#sum(sapply(l1coeffs, function(i) sum(i != 0)))
length(unique(as.vector(sapply(l1coeffs, function(i) rownames(i)[which(i != 0)]))))-1
```

A total of 147 (pixel) predictors are selected. Since the multinomial logistic regression implementation in glmnet fits a total of 10 equations, we have a total of 1480 coefficients estimated for this choice of lambda, including intercepts.

4. Repeat parts (1)-(3) with type.multinomial="ungrouped".

```{r}
set.seed(665)
g2 <- cv.glmnet(as.matrix(train[,-257]), factor(train[,257]), family="multinomial", alpha=1, type.measure="class", 
                type.multinomial="ungrouped")
plot(g2)
```

```{r}
l2.1se <- g2$lambda.1se
l2.min <- g2$lambda.min
l2.min
l2.1se

p2.1se <- predict(g2, s=l2.1se, newx = as.matrix(valid[,-257]), type="class")
p2.min <- predict(g2, s=l2.min, newx = as.matrix(valid[,-257]), type="class")
mean(p2.1se != valid[,257])
mean(p2.min != valid[,257])

l2coeffs <- predict(g2, s=l2.1se, type="coefficients")
#sapply(l2coeffs, function(i) sum(i != 0))
#sum(sapply(l2coeffs, function(i) sum(i != 0)))
length(unique(unlist(sapply(l2coeffs, function(i) rownames(i)[which(i != 0)]))))-1
```


# Part 2

Apply the ridge penalty to train a multinomial logistic regression classifier. (I would not use the
type.multinomial= argument from cv.glmnet() because thereâ€™s no need for that here.) Again, use
10-fold cross-validation on the training set to select your $\lambda$. Show a plot that displays the misclassification
error on the y-axis for values of $\log(\lambda)$ on the x-axis.

```{r}
set.seed(665)
g3 <- cv.glmnet(as.matrix(train[,-257]), factor(train[,257]), family="multinomial", alpha=0, type.measure="class")
plot(g3)
```












